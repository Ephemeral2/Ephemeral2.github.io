<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ephemeral</title>
  
  <subtitle>Live Fast, Die Young, Be Wild, Have Fun</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-10-24T03:34:32.136Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Ephemeral</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>特征分解</title>
    <link href="http://example.com/2022/10/24/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3/"/>
    <id>http://example.com/2022/10/24/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3/</id>
    <published>2022-10-24T03:34:17.000Z</published>
    <updated>2022-10-24T03:34:32.136Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特征分解"><a href="#特征分解" class="headerlink" title="特征分解"></a>特征分解</h2><h2 id="奇异值分解-Singular-value-decomposition"><a href="#奇异值分解-Singular-value-decomposition" class="headerlink" title="奇异值分解 Singular value decomposition"></a>奇异值分解 Singular value decomposition</h2><p>奇异值分解，相比于特征分解可以在矩阵不为方阵的条件下对矩阵进行分解。</p><p>假设矩阵A是一个m*n的矩阵，则定义矩阵A的SVD为：</p><script type="math/tex; mode=display">A = U\Sigma V^T= (orthogonal)(diagonal)(orthogonal)</script><p>$其中U是m×m的矩阵，\Sigma 是m×n的对角矩阵（主对角线上为奇异值，除主对角线外皆为0），V是n×n矩阵；\\且U、V均满足U^TU = I,V^TV = I$ </p><p><img src="https://pic4.zhimg.com/v2-5ee98f8f3426b845bc1c5038ecd29593_r.jpg" alt="img"></p><p>矩阵U：$(AA^T)u_i=\lambda_iu_i$，讲m个特征向量v张成一个$m×m$的矩阵U</p><p>矩阵V：$(A^TA)v_i=\lambda_iv_i$，讲n个特征向量v张成一个$n×n$的矩阵V</p><p>奇异值矩阵$\Sigma$：$A=U\Sigma V^T \Rightarrow AV=U\Sigma V^TV \Rightarrow AV=U\Sigma\\\Rightarrow Av_i=\sigma_iu_i\Rightarrow\sigma_i=Av_i/u_i$ </p><h3 id="Proof："><a href="#Proof：" class="headerlink" title="Proof："></a>Proof：</h3><script type="math/tex; mode=display">A=U\Sigma V^T \Rightarrow  A^T=V\Sigma U^T \Rightarrow A^TA = V\Sigma U^TU\Sigma V^T = V\Sigma^2V^T</script><p> 上式证明了$AA^T$的特征向量即为SVD中的U矩阵，V矩阵同理；</p><p>$U^TU=I,\Sigma^T=\Sigma$；</p><p>同时可证明特征值和奇异值满足如下关系：$\sigma_i= \sqrt{\lambda_i}$，即通过求出$A^TA$的特征值取平方的方式求得奇异值</p><h2 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h2><h2 id="LASSO"><a href="#LASSO" class="headerlink" title="LASSO"></a>LASSO</h2><h2 id="Kernel-Methods"><a href="#Kernel-Methods" class="headerlink" title="Kernel Methods"></a>Kernel Methods</h2><p>高维空间比低维空间更易线性可分</p><p><strong>核函数</strong>：$K:X×X\rightarrow R,\forall X,Z 属于X,则K(X,Z)为核函数$</p><p><strong>正定核函数：</strong>$K:X×X\rightarrow R,\forall X,Z 属于X,则K(X,Z)为核函数\\如果\exist:\phi：X\rightarrow R, \in H（希尔伯特空间）$</p><p><strong>Hilbert Space:</strong> 完备的，可能是无限维的，被赋予内积运算的线性空间</p><ul><li><p>线性空间：向量空间（加法和数乘）</p></li><li><p>完备：对极限操作是封闭的（即：若$K_n\in H，且\lim_{n \to \infty}\in H，则H为封闭空间$）</p></li><li><p>内积：</p><ul><li><p>对称性: $<f,g>=<g,f>$</p></li><li><p>正定性: $<f,f>\ge 0，当=成立时\Leftrightarrow f=0$</p></li><li><p>线性: $<r_1f_1+r_2f_2,g>=r_1<f_1,g>+r_2<f_2,g>$</p></li></ul></li></ul><p><strong>$证明：K(X,Z)=&lt;\phi (X),\phi (Z)&gt; \Leftrightarrow Gram\ matrix 为半正定$</strong></p><ul><li><p>必要性证明：</p><p>$\because K(X,Z)=&lt;\phi(X),\phi(Z)&gt;,K(Z,X)=&lt;\phi(Z),\phi(X)&gt;$</p></li></ul><p>​        $\because 内积具有对称性质，即&lt;\phi(X),\phi(Z)&gt;=&lt;\phi(Z),\phi(X)&gt;$</p><p>​        $\therefore K(X,Z) = K(Z,X)$</p><p>​        $\therefore K(X,Z)满足对称性$    </p><ul><li><p>充分性证明：</p><p>$欲证Gram\ Matrix 为半正定矩阵，即证：\forall \alpha\in R^N, \alpha^T K \alpha \ge 0$</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221021145655547.png" alt="image-20221021145655547"></p></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>AMA546 chapter 4</p><p><a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/132275334">深入浅出说说ridge regression - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/51431045">回归分析|笔记整理（A）——岭回归，主成分回归（上） - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/67986077">[深度概念]·K-Fold 交叉验证 (Cross-Validation)的理解与应用 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/weixin_38393494/article/details/112648438?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-112648438-blog-80584430.pc_relevant_3mothn_strategy_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-112648438-blog-80584430.pc_relevant_3mothn_strategy_recovery&amp;utm_relevant_index=2">(28条消息) 【笔记】普通交叉验证 (CV) ，广义交叉验证(GCV)，图像恢复正则化参数选择_木木mum的博客-CSDN博客_广义交叉验证</a></p><p><a href="https://www.bilibili.com/video/av34731384/?p=1&amp;vd_source=20d1ec2b241b91d79fba2e1488e1c7bc">机器学习-核方法（1）-背景介绍_哔哩哔哩_bilibili</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;特征分解&quot;&gt;&lt;a href=&quot;#特征分解&quot; class=&quot;headerlink&quot; title=&quot;特征分解&quot;&gt;&lt;/a&gt;特征分解&lt;/h2&gt;&lt;h2 id=&quot;奇异值分解-Singular-value-decomposition&quot;&gt;&lt;a href=&quot;#奇异值分解-Singu</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>分类算法 Classification</title>
    <link href="http://example.com/2022/10/24/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-Classification/"/>
    <id>http://example.com/2022/10/24/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-Classification/</id>
    <published>2022-10-24T03:33:12.000Z</published>
    <updated>2022-10-24T03:33:52.114Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分类算法-Classification"><a href="#分类算法-Classification" class="headerlink" title="分类算法 Classification"></a>分类算法 Classification</h1><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><h3 id="决策树-Decision-Trees"><a href="#决策树-Decision-Trees" class="headerlink" title="决策树 Decision Trees"></a>决策树 Decision Trees</h3><p>决策树是一种监督学习（Supervised Learning）</p><p>监督学习：给出一组样本，每个样本都有一组属性和一个分类结果，即分类结果已知；通过学习这些样本得到一个决策树，这个决策树能够对新的数据给出正确的分类。</p><h4 id="Hunt-算法"><a href="#Hunt-算法" class="headerlink" title="Hunt 算法"></a>Hunt 算法</h4><p>通过将训练记录相继划分成较纯的子集，以递归方式建立决策树</p><ul><li><p>Hunt算法没有全局最优解</p></li><li><p>贪心策略可以用于支持</p></li><li><p>指定测试条件的方法：</p><ul><li>属性类型：Nominal, ordinal, continuous</li><li>分割方式：2-way split, Multi-way split</li></ul></li><li><p>判断是否是最优分割的方式：随着划分过程的不断进行，我们希望决策树的分支节点包含的样本尽可能属于同一类，即节点的纯度越来越高。纯度越高，类分布就越倾斜，划分结果越好。</p><ul><li><p>纯度 Impurity</p><p><img src="https://img-blog.csdnimg.cn/img_convert/ab540e9ca5ec29eb0d5025faf50883c4.png#pic_center" alt="img"></p><p>$当C_0类的数量占总数的90\%时该节点的纯度较大。 $</p><p>为了确定按某个属性划分的效果，我们需要比较划分前(父亲节点)和划分后(所有儿子节点)不纯度的降低程度，降低越多，划分的效果就越好。</p></li></ul></li></ul><h4 id="信息增益法-Information-gain-ID3"><a href="#信息增益法-Information-gain-ID3" class="headerlink" title="信息增益法 Information gain (ID3)"></a>信息增益法 Information gain (ID3)</h4><ul><li><strong>信息熵：</strong>一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件不确定的事就需要了解大量信息。<strong>熵(entropy)用于表示随机变量不确定性的度量</strong>，熵越大，表示不确定性越大。</li><li>$假设变量S有S_i(i=1,2,…,n)种情况，p_i表示第i种情况的概率，那么随机变量S的熵定义为：$</li></ul><script type="math/tex; mode=display">Entropy(S)=-\sum_{i=1}^{c} p_i\log_2p_i</script><p>Example：若14个值内部有9个“是”，5个“否”，则该数据“购买”的熵为</p><script type="math/tex; mode=display">Infor（D）=I（9,5）=-\frac{9}{14}\log_2{\frac{9}{14}}-\frac{5}{14}\log_2{\frac{5}{14}}=0.940</script><p>在随机变量S给定的条件下，随机变量A的条件熵Entropy(A|S)定义为：</p><script type="math/tex; mode=display">Entropy(A|S)=-\sum_{i=1}^{c} p_i·Entropy(A|S=s_i)</script><p><strong>信息增益</strong>：得知特征X的信息而使得分类Y的信息的不确定性减少的程度<em>（由于X的存在使得Y没那么不确定了）</em>。如果某个特征的信息增益比较大，就表示该特征对结果的影响较大，特征A对数据集S的信息增益表示为：</p><script type="math/tex; mode=display">Gain(S,A)=Entropy(S)-\sum_{v\in A}\frac{|S_v|}{|S|} Entropy(S_v)</script><div class="table-container"><table><thead><tr><th>青年人（5）</th><th>中年人（4）</th><th>老年人(5)</th></tr></thead><tbody><tr><td>购买（2）</td><td>购买（4）</td><td>购买（3）</td></tr><tr><td>不购买（3）</td><td>不购买（0）</td><td>不购买（2）</td></tr></tbody></table></div><p>$I(2,3)=$</p><p>$I(4,0)=$</p><p>$I(3,2)=$</p><p>$Infor_{age}(D)=\frac{5}{14}I(2,3)+\frac{4}{14}I(0,4)+\frac{5}{14}I(3,2)=0.694$</p><p>$Gain(age)=Info(D)-Info_age(D)=0.940-0694=0.246$</p><p>同理可得：</p><p>$Gain(age)=0.246$</p><p>$Gain(income)=0.029$</p><p>$Gain(fancy)=0.151$</p><p>$Gain(credit_rating)=0.048$</p><h3 id="朴素贝叶斯分类器-Naive-Bayes-Classifier"><a href="#朴素贝叶斯分类器-Naive-Bayes-Classifier" class="headerlink" title="朴素贝叶斯分类器 Naive Bayes Classifier"></a>朴素贝叶斯分类器 Naive Bayes Classifier</h3><ul><li><strong>贝叶斯派的核心思想：</strong>支持某项属性的事件发生得愈多，则该属性成立的可能性就愈大。</li></ul><p>​        贝叶斯方法建立在主观判断的基础上，你可以先估计一个值，然后根据客观事实不断修正。</p><ul><li><strong>贝叶斯公式：</strong>$P(A|B)=P(B|A)*P(A)/P(B)$<ul><li>$P(A|B):（后验概率）B事件发生的条件下A事件发生的条件概率$</li><li>$P(B|A):A事件发生的条件下B事件发生的条件概率$</li><li>$P(A):（先验概率）$</li></ul></li><li><p><strong>贝叶斯分类器：</strong></p></li><li><p><strong>朴素贝叶斯：</strong>假设特征之间相互独立</p></li></ul><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://zhuanlan.zhihu.com/p/30059442">决策树(Decision Tree)：通俗易懂之介绍 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/qq_43753724/article/details/125208042">(29条消息) 决策树(Hunt、ID3、C4.5、CART)_别团等shy哥发育的博客-CSDN博客_hunt决策树</a></p><p><a href="https://zhuanlan.zhihu.com/p/37575364">一文详解朴素贝叶斯(Naive Bayes)原理 - 知乎 (zhihu.com)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;分类算法-Classification&quot;&gt;&lt;a href=&quot;#分类算法-Classification&quot; class=&quot;headerlink&quot; title=&quot;分类算法 Classification&quot;&gt;&lt;/a&gt;分类算法 Classification&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Neutral Network</title>
    <link href="http://example.com/2022/03/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C+%E5%85%A8%E8%BF%9E%E6%8E%A5+Tahn+Relu/"/>
    <id>http://example.com/2022/03/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C+%E5%85%A8%E8%BF%9E%E6%8E%A5+Tahn+Relu/</id>
    <published>2022-03-29T16:00:00.000Z</published>
    <updated>2022-03-30T04:33:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h1><h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><p>​    神经网络由大量的神经元相互连接而成。每个神经元接受线性组合的输入后，最开始只是简单的线性加权，后来给每个神经元加上了非线性的激活函数，从而进行非线性变换后输出。每两个神经元之间的连接代表加权值，称之为<strong>权重（weight）</strong>。不同的权重和激活函数，则会导致神经网络不同的输出。</p><p>​    举个手写识别的例子，给定一个未知数字，让神经网络识别是什么数字。此时的神经网络的输入由一组被输入图像的像素所激活的输入神经元所定义。在通过非线性激活函数进行非线性变换后，神经元被激活然后被传递到其他神经元。重复这一过程，直到最后一个输出神经元被激活。从而识别当前数字是什么字。<script type="math/tex">\omega*X+b</script></p><p><img src="https://img-blog.csdn.net/20160716131107406" alt="img"></p><p>基本 $wx + b$ 的形式，其中</p><ul><li>$x1,x2$  表示输入向量</li><li>$w1,w2$  为权重，几个输入则意味着有几个权重，即每个输入都被赋予一个权重</li><li><em>b</em>为偏置bias</li><li>$g(z)$ 为激活函数</li><li><em>a</em> 为输出</li></ul><p>​    一开始为了简单，人们把激活函数定义成一个线性函数，即对于结果做一个线性变化，比如一个简单的线性激活函数是g(z) = z，输出都是输入的线性变换。后来实际应用中发现，线性激活函数太过局限，于是人们引入了非线性激活函数。</p><h2 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h2><p>​    将上文的神经元结合在一起便组成了神经网络，下图是一个三层神经网络。</p><p>​    输入层(样本数据)<br>​    隐藏层(隐藏层的层数和每层的神经元数目需要自己给定)， 如果有多个隐藏层，则意味着多个激活函数。<br>​    输出层(预测目标)</p><p><img src="https://img-blog.csdnimg.cn/20190101123204625.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTcyMzQ2NA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" /></p><p>​    同时，每一层都可能由单个或多个神经元组成，每一层的输出将会作为下一层的输入数据。比如下图中间隐藏层来说，隐藏层的3个神经元a1、a2、a3皆各自接受来自多个不同权重的输入（因为有x1、x2、x3这三个输入，所以a1 a2 a3都会接受x1 x2 x3各自分别赋予的权重，即几个输入则几个权重），接着，a1、a2、a3又在自身各自不同权重的影响下 成为的输出层的输入，最终由输出层输出最终结果。<img src="https://img-blog.csdn.net/20160703110336151" alt="img"></p><p>​    $a_i^j$ 表示第j层第i个单元的激活函数/神经元</p><p>​    $\Theta^{(j)}$ 表示从第j层映射到第j+1层的控制函数的权重矩阵 </p><p>​    此外，输入层和隐藏层都存在一个偏置（bias unit)，所以上图中也增加了偏置项：x0、a0。针对上图，有如下公式</p><script type="math/tex; mode=display">a_1^{(2)} = g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3)\\a_2^{(2)} = g(\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3)\\a_3^{(2)} = g(\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3)\\h_{\Theta}(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)}+\Theta_{11}^{(2)}a_1^{(2)}+\Theta_{12}^{(2)}a_2^{(2)}+\Theta_{13}^{(2)}a_3^{(2)})</script><h3 id="卷积神经网络的层级结构"><a href="#卷积神经网络的层级结构" class="headerlink" title="卷积神经网络的层级结构"></a>卷积神经网络的层级结构</h3><p><img src="https://img-blog.csdn.net/20160702205047459" alt="img"></p><p>最左边是：</p><p>​    数据输入层，在该层对数据做一些处理，比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）、归一化（把所有的数据都归一到同样的范围）、PCA/白化等等。CNN只对训练集做“去均值”这一步。</p><p>中间是：</p><p>​    CONV：卷积计算层，线性乘积 求和。<br>​    RELU：激励层<br>​    POOL：池化层，简言之，即取区域平均或最大。</p><p>最右边是：</p><p>​    FC：全连接层</p><h3 id="CNN的卷积计算层"><a href="#CNN的卷积计算层" class="headerlink" title="CNN的卷积计算层"></a>CNN的卷积计算层</h3><h4 id="卷积："><a href="#卷积：" class="headerlink" title="卷积："></a>卷积：</h4><p>​    对不同的数据窗口数据和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做<strong>内积</strong>（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。</p><p>​    如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据</p><p><img src="https://img-blog.csdn.net/20160702215705128" alt="img"></p><p>$4<em>0 + 0</em>0 + 0<em>0 + 0</em>0 + 0<em>1 + 0</em>1 + 0<em>0 + 0</em>1 + -4*2 = -8$</p><p>​    在CNN中，滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数：<br>　　a. 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。</p><p>​        b. 步长stride：决定滑动多少步可以到边缘。</p><p>​    　c. 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。 </p><p><img src="https://img-blog.csdn.net/20160705162205761" alt="img"></p><p><img src="https://img-blog.csdn.net/20160707204048899" alt="img"></p><h4 id="激励层与池化层"><a href="#激励层与池化层" class="headerlink" title="激励层与池化层"></a>激励层与池化层</h4><h5 id="激励："><a href="#激励：" class="headerlink" title="激励："></a>激励：</h5><p>实际梯度下降中，sigmoid容易饱和、造成终止梯度传递，且没有0中心化。因此可以尝试另外一个激活函数：ReLU 优点是收敛快，求梯度简单。</p><h5 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h5><p>​    下图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。<img src="https://img-blog.csdn.net/20160703121026432" alt="img"></p><h2 id="非线性函数-激活函数"><a href="#非线性函数-激活函数" class="headerlink" title="非线性函数(激活函数)"></a>非线性函数(激活函数)</h2><p>​    如果把神经元的非线性函数去掉的话，那么这个神经元可以写成：<strong>输出=输入*权值+偏置</strong>。这个就是<em>线性回归方程</em>，所以如果把神经网络的非线性函数去掉，那么<em>整个网络就是由多个线性回归</em>组成的。线性回归是线性方程，只能解决线性可分的问题。多个线性回归组成的结果无论经过多少的层的神经网络，其本质上也是线性方程。但现实中的很多需要解决的问题都是线性不可分的(异或问题)。<em>*所以为了解决这些线性不可分的问题，我们引入了非线性方程 g</em>，那么输出就变为了 <script type="math/tex">g(\omega*X+b)</script>。</p><h3 id="目前常用的非线性函数"><a href="#目前常用的非线性函数" class="headerlink" title="目前常用的非线性函数"></a>目前常用的非线性函数</h3><h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数:"></a>sigmoid函数:</h4><script type="math/tex; mode=display">S(x) = \frac{1}{1+e^{-x}}</script><p><img src="https://img-blog.csdnimg.cn/20190101125917773.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTcyMzQ2NA==,size_16,color_FFFFFF,t_70" alt="img"></p><p>​    sigmoid函数把求和的结果都变为(0,1)之间的值，压缩至0到1有何用处呢？用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。</p><h4 id="tanh函数："><a href="#tanh函数：" class="headerlink" title="tanh函数："></a>tanh函数：</h4><script type="math/tex; mode=display">tanh(x) = \frac{sinh(x)}{cosh(x)}=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><p><img src="https://img-blog.csdnimg.cn/2019010113063249.png" alt="img"></p><p>​    在具体应用中，tanh函数相比于Sigmoid函数往往更具有优越性，这主要是因为<strong>Sigmoid函数在输入处于[-1,1]之间时，函数值变化敏感</strong>，一旦接近或者超出区间就失去敏感性，处于饱和状态，影响神经网络预测的精度值。主要体现在计算梯度时。</p><h4 id="relu函数："><a href="#relu函数：" class="headerlink" title="relu函数："></a>relu函数：</h4><script type="math/tex; mode=display">f(x) = max(0,x)</script><p><img src="https://img-blog.csdnimg.cn/20190101131458349.png" alt="img"></p><p>​    relu函数并未像tanh函数和sigmoid函数一样，将求和的值限制在(0,1)之间。其阈值是[0,+∞]。</p><p>​    上面3个激活函数主要是用在从输入层到隐藏层以及隐藏层到隐藏层之间。</p><p>​    对于最后一层隐藏层到输出层之间，一般选用softmax函数进行归一化。</p><h4 id="softmax函数："><a href="#softmax函数：" class="headerlink" title="softmax函数："></a>softmax函数：</h4><script type="math/tex; mode=display">\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}}\\\\for j = 1,...,K</script><p>​    由于在用神经网络解决的问题大部分都是多分类问题，softmax是将多分类转化为概率的一个函数。如果预测的目标是二分类，则使用sigmoid进行二分类的概率转化。</p><p>上面表达式中的 $z^j$ = 隐藏层的输入<em>权重+偏置，</em>K*为预测目标的分类数量。</p><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>​        全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。</p><p>​        全连接层每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。</p><p>​        <img src="https://img-blog.csdnimg.cn/20190325192219666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxOTk3OTIw,size_16,color_FFFFFF,t_70" alt="img"></p><p>​    从左至右，一张彩色图片输入到网络，白色框是卷积层，红色是池化，蓝色是全连接层，棕色框是预测层。预测层的作用是将全连接层输出的信息转化为相应的类别概率，而起到分类作用。    </p><p>​    如果说卷积层、池化层和激活函数层（预测层）等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示<strong>（7x7x512）</strong>”映射到样本标记空间<strong>(三个全连接层)</strong>的作用。在实际使用中，全连接层可由卷积操作实现：</p><p>​    对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为$h<em>w$的全局卷积，h和w分别为前层卷积结果的高和宽<em>*（7x7）</em></em>。</p><p>​    全连接的核心操作就是矩阵向量乘积 $ y = Wx$</p><p>​    本质就是由一个特征空间线性变换到另一个特征空间。目标空间的任一维 (也就是隐层的一个 cell)都认为会受到源空间的每一维的影响。不考虑严谨，可以说，<strong>目标向量是源向量的加权和</strong>。</p><p>​    在 CNN 中，全连接常出现在最后几层，用于对前面设计的特征做加权和。比如 mnist，前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。<u>在 RNN 中，全连接用来把 embedding 空间拉到隐层空间，把隐层空间转回 label 空间等</u>。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://blog.csdn.net/qq_41997920/article/details/88803736?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_default&amp;utm_relevant_index=5">https://blog.csdn.net/qq_41997920/article/details/88803736?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_default&amp;utm_relevant_index=5</a></p><p><a href="https://y1ran.blog.csdn.net/article/details/81385159?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.topblog&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.topblog&amp;utm_relevant_index=6">https://y1ran.blog.csdn.net/article/details/81385159?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.topblog&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.topblog&amp;utm_relevant_index=6</a></p><p><a href="https://blog.csdn.net/v_JULY_v/article/details/51812459?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164860614516782089398149%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164860614516782089398149&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-51812459.142">https://blog.csdn.net/v_JULY_v/article/details/51812459?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164860614516782089398149%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164860614516782089398149&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-51812459.142</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;人工神经网络&quot;&gt;&lt;a href=&quot;#人工神经网络&quot; class=&quot;headerlink&quot; title=&quot;人工神经网络&quot;&gt;&lt;/a&gt;人工神经网络&lt;/h1&gt;&lt;h2 id=&quot;神经元&quot;&gt;&lt;a href=&quot;#神经元&quot; class=&quot;headerlink&quot; title=&quot;神经元</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hexo博客部署</title>
    <link href="http://example.com/2022/03/29/Hexo%E5%8D%9A%E5%AE%A2%E9%83%A8%E7%BD%B2/"/>
    <id>http://example.com/2022/03/29/Hexo%E5%8D%9A%E5%AE%A2%E9%83%A8%E7%BD%B2/</id>
    <published>2022-03-29T15:10:55.000Z</published>
    <updated>2022-03-30T02:26:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用HEXO实现个人博客"><a href="#使用HEXO实现个人博客" class="headerlink" title="使用HEXO实现个人博客"></a>使用HEXO实现个人博客</h1><p>Node.js 安装<br>npm -v<br>npm install -g cnpm —registry=<a href="https://registry.npm.taobao.org">https://registry.npm.taobao.org</a><br>cnpm install -g hexo-cli<br>hexo -v<br>C:\Users\21059165g&gt;mkdir My_Blog<br>hexo init<br>hexo s<br><a href="http://localhost:4000/">http://localhost:4000/</a><br>hexo n “post_name”<br>也可以创建md文件再复制到文件夹下<br>cd source/_posts</p><h2 id="将个人博客推到github"><a href="#将个人博客推到github" class="headerlink" title="将个人博客推到github"></a>将个人博客推到github</h2><p>C:\Users\21059165g\My_Blog&gt;cnpm install —save hexo-deployer-git<br>打开 _config.yml<br>deploy:<br>  type: git<br>  repo: <a href="https://github.com/HANE-iwnl/HANE-iwnl/_posts.github.io.git">https://github.com/HANE-iwnl/HANE-iwnl/_posts.github.io.git</a><br>  branch :master<br>hexo d</p><p>git clone <a href="https://github.com/litten/hexo-theme-yilia.git">https://github.com/litten/hexo-theme-yilia.git</a> themes/yilia</p><p>Hexo clean<br>Hexo g<br>Hexo s</p><h2 id="使hexo支持数学公式"><a href="#使hexo支持数学公式" class="headerlink" title="使hexo支持数学公式"></a>使hexo支持数学公式</h2><p>npm install hexo-math —save</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plugins:</span><br><span class="line">    - markdown-it-footnote</span><br><span class="line">    - markdown-it-sup</span><br><span class="line">    - markdown-it-sub</span><br><span class="line">    - markdown-it-abbr</span><br><span class="line">    - markdown-it-emoji</span><br><span class="line">    - hexo-math</span><br></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br><span class="line">  cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML</span><br><span class="line"># Han Support docs: https://hanzi.pro/</span><br></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: A Title</span><br><span class="line">date: 2020-02-08 10:39:55</span><br><span class="line">tags:</span><br><span class="line">- tag1</span><br><span class="line">- tag2</span><br><span class="line">categories:</span><br><span class="line">- parent</span><br><span class="line">- child</span><br><span class="line"></span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用HEXO实现个人博客&quot;&gt;&lt;a href=&quot;#使用HEXO实现个人博客&quot; class=&quot;headerlink&quot; title=&quot;使用HEXO实现个人博客&quot;&gt;&lt;/a&gt;使用HEXO实现个人博客&lt;/h1&gt;&lt;p&gt;Node.js 安装&lt;br&gt;npm -v&lt;br&gt;npm in</summary>
      
    
    
    
    
  </entry>
  
</feed>
