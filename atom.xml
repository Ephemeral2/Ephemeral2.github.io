<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ephemeral</title>
  
  <subtitle>Live Fast, Die Young, Be Wild, Have Fun</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-12-06T09:57:42.328Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Ephemeral</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>statistical data mining</title>
    <link href="http://example.com/2022/12/02/Statistical-Data-Mining/"/>
    <id>http://example.com/2022/12/02/Statistical-Data-Mining/</id>
    <published>2022-12-02T07:20:21.000Z</published>
    <updated>2022-12-06T09:57:42.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><h2 id="0-1-Derivatives"><a href="#0-1-Derivatives" class="headerlink" title="0.1. Derivatives"></a>0.1. Derivatives</h2><p>设向量$X为\begin{pmatrix} X_1\\X_2\\X_3\\…\\X_p \end{pmatrix}\$</p><h2 id="0-2-Taylor"><a href="#0-2-Taylor" class="headerlink" title="0.2. Taylor"></a>0.2. Taylor</h2><h1 id="Chapter-1"><a href="#Chapter-1" class="headerlink" title="Chapter 1"></a>Chapter 1</h1><h1 id="Chapter-2"><a href="#Chapter-2" class="headerlink" title="Chapter 2"></a>Chapter 2</h1><h1 id="Chapter-3"><a href="#Chapter-3" class="headerlink" title="Chapter 3"></a>Chapter 3</h1><h1 id="Chapter-4"><a href="#Chapter-4" class="headerlink" title="Chapter 4"></a>Chapter 4</h1><h2 id="4-1-Singular-Value-Decomposition"><a href="#4-1-Singular-Value-Decomposition" class="headerlink" title="4.1. Singular Value Decomposition"></a>4.1. Singular Value Decomposition</h2><h2 id="4-2-Least-Squares"><a href="#4-2-Least-Squares" class="headerlink" title="4.2. Least Squares"></a>4.2. Least Squares</h2><h2 id="4-3-Ridge-Regression"><a href="#4-3-Ridge-Regression" class="headerlink" title="4.3. Ridge Regression"></a>4.3. Ridge Regression</h2><h2 id="4-4-Lasso"><a href="#4-4-Lasso" class="headerlink" title="4.4. Lasso"></a>4.4. Lasso</h2><h1 id="Chapter-5"><a href="#Chapter-5" class="headerlink" title="Chapter 5"></a>Chapter 5</h1><h2 id="5-1-Classification"><a href="#5-1-Classification" class="headerlink" title="5.1 Classification"></a>5.1 Classification</h2><p><strong>分类（classification）</strong>：分类任务就是通过学习得到一个目标函数$f$（或分类模型），把每个属性集x映射到一个预先定义的类标签（Label）$y$。</p><p><strong>建立分类模型的一般方法：</strong></p><p><img src="https://img-blog.csdn.net/20180119092903406?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVE9NT0NBVA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p><h2 id="5-2-Decision-Tree"><a href="#5-2-Decision-Tree" class="headerlink" title="5.2 Decision Tree"></a>5.2 Decision Tree</h2><h3 id="5-2-1-结点："><a href="#5-2-1-结点：" class="headerlink" title="5.2.1 结点："></a>5.2.1 结点：</h3><p><strong>决策树中包含三种结点：</strong></p><ul><li>根结点（root node）: the first criterion. No incoming edge.</li><li>内部结点（internal node）: all the other criteria. Only one incoming edge, two or more outgoing edges.</li><li>叶结点（leaf node）: one incoming edge and no outgoing edge.</li></ul><h3 id="5-2-2-Hunt-Algorithm："><a href="#5-2-2-Hunt-Algorithm：" class="headerlink" title="5.2.2 Hunt Algorithm："></a>5.2.2 Hunt Algorithm：</h3><p>通过将训练记录相继划分为纯度较高的子集，以递归的方式建立决策树。</p><p>设$D_t$是与结点t相关联的训练集/训练数据，而$y={y_1,y_2,…,y_c}$是类标号，Hunt算法的递归定义如下：</p><ul><li>如果$D_t$中所有记录都属于同一个类$y_t$，则t是叶结点，用$y_t$标记。</li><li>如果$D_t$中包含属于多个类的记录，则选择一个属性，并利用该属性将记录划分为较小的子集。对于测试条件的每个输出，创建一个子结点，并根据测试结果将$D_t$中的记录分布到子结点中。然后，对每个子女结点，递归调用该算法。</li></ul><p>第二步所创建的子结点可能为空，即不存在与这些结点相关联的记录。该节点成为叶结点，类标号为其父结点上训练记录中的多数类。</p><p>在第二步，如果与$D_t$相关联的所有记录都具有相同属性值（目标属性除外），则不可能进一步划分这些记录。这种情况下，该结点为叶结点，其标号为与该结点相关联的训练记录中的多数类。</p><h3 id="5-2-3-表示属性测试条件的方法"><a href="#5-2-3-表示属性测试条件的方法" class="headerlink" title="5.2.3 表示属性测试条件的方法"></a>5.2.3 <strong>表示属性测试条件的方法</strong></h3><p><strong>二元属性：</strong>二元属性的测试条件产生两个可能的输出。</p><p><strong>标称属性：</strong></p><ul><li>多路划分（Multi-way split）：其输出数取决于该属性不同属性值的个数。</li><li>二元划分（Binary (2-way) split）：某些决策树算法（如CART）只产生二元划分，它们考虑创建k个属性值的二元划分的所有$2^{(k-1)} -1$种方法。</li></ul><p><strong>序数属性：</strong>序数属性也可以产生二元或多路划分，只要不违背序数属性值的有序性，就可以对属性值进行分组。</p><p><strong>连续属性：</strong>to be continued…</p><h3 id="5-2-4-选择最佳划分的度量"><a href="#5-2-4-选择最佳划分的度量" class="headerlink" title="5.2.4 选择最佳划分的度量"></a>5.2.4 <strong>选择最佳划分的度量</strong></h3><p>选择最佳划分的度量通常是根据划分后子女结点不纯性的程度。</p><h4 id="5-2-4-1-Measures-of-Impurity"><a href="#5-2-4-1-Measures-of-Impurity" class="headerlink" title="5.2.4.1 Measures of Impurity"></a>5.2.4.1 Measures of Impurity</h4><p>​    <img src="https://pic4.zhimg.com/80/v2-e5cddc4aae60bd7a14a8089afb0e5453_1440w.webp" alt="img"></p><p>​    为了确定测试条件的效果，我们需要比较父结点（划分前）的不纯程度和子结点（划分后）的不纯程度，它们的差越大，测试条件的效果就越好。增益 $Δ $：</p><p>​    <img src="https://pic1.zhimg.com/80/v2-dc00c81ccff5b057185c5f07dbc207e8_1440w.webp" alt="img">    </p><p>决策树算法通常选择最大化增益 $Δ $的测试条件。因为对所有的测试条件来说，$I(parent)$是一样的，所以<strong>最大化增益等价于最小化子结点的不纯性度量的加权平均值</strong>。特别地，当选择熵作为上述增益公式的不纯性度量时，称该增益为<strong>信息增益</strong>。</p><h3 id="5-2-5-Model-over-fitting"><a href="#5-2-5-Model-over-fitting" class="headerlink" title="5.2.5 Model over-fitting"></a>5.2.5 Model over-fitting</h3><p>分类模型的误差大致分为两种：训练误差和泛化误差。</p><p>拟合不足：训练误差与检验误差都很大</p><p>过分拟合：训练误差下降但是检验误差上升</p><p><img src="https://img-blog.csdn.net/20180909163503505?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RPTU9DQVQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>过拟合的原因：</p><ol><li><p>噪声导致的过拟合：样本被错误标记。</p></li><li><p>缺乏代表性样本导致的过分拟合：根据少量训练记录做出分类决策的模型也容易受过分拟合的影响。</p></li></ol><h4 id="5-2-5-1-解决过拟合的方式"><a href="#5-2-5-1-解决过拟合的方式" class="headerlink" title="5.2.5.1 解决过拟合的方式"></a>5.2.5.1 解决过拟合的方式</h4><ul><li>Pre-pruning:  预剪枝要对划分前后的泛化性能进行估计来决定是否要进行这个划分</li><li>Post-pruning:<ul><li>分析剪枝前后泛化能力（验证集精度）</li><li>奥卡姆剃刀原则：虽然剪枝后精度并未提升，但是模型简化了也需要剪枝。</li></ul></li></ul><h4 id="5-2-5-2-ID3-algorithm"><a href="#5-2-5-2-ID3-algorithm" class="headerlink" title="5.2.5.2 ID3 algorithm"></a>5.2.5.2 ID3 algorithm</h4><p>信息增益是特征选择中的一个重要指标，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。</p><p>基本信息包括：熵，期望信息和信息增益。</p><ol><li>熵：设D为用类别对训练元组进行的划分，则D的熵表示为：</li></ol><p><img src="https://img-blog.csdn.net/20180119095021166?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVE9NT0NBVA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p><p>其中$p_i$表示第i个类别在整个训练元组中出现的概率，可以用属于此类别元素的数量除以训练元组元素总数量作为估计。熵的实际意义表示是D中元组的类标号所需要的平均信息量。</p><ol><li>期望信息：</li></ol><p><img src="https://img-blog.csdn.net/20180119095411084?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVE9NT0NBVA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p><p>现在我们假设将训练元组D按属性A进行划分，则A对D划分的期望信息为：</p><ol><li>信息增益：<img src="https://img-blog.csdn.net/20180119095449253?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvVE9NT0NBVA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></li></ol><h3 id="5-2-6-Random-Forest"><a href="#5-2-6-Random-Forest" class="headerlink" title="5.2.6 Random Forest"></a>5.2.6 Random Forest</h3><p>​        随机森林是一种以决策树为基分类器的集成算法，通过组合多棵独立的决策树后根据投票或取均值的方式得到最终预测结果的机器学习方法，往往比单棵树具有更高的准确率和更强的稳定性。</p><p>​        随机森林相比于决策树拥有出色的性能主要取决于随机抽取样本和特征和集成算法，前者让它具有更稳定的抗过拟合能力，后者让它有更高的准确率。</p><h2 id="5-3-Naive-Bayes-classifier"><a href="#5-3-Naive-Bayes-classifier" class="headerlink" title="5.3 Naive Bayes classifier"></a>5.3 Naive Bayes classifier</h2><p><a href="https://blog.csdn.net/TOMOCAT/article/details/82934726">(36条消息) [DataAnalysis]贝叶斯分类器_TOMOCAT的博客-CSDN博客_corrplot</a></p><h3 id="5-3-1-条件概率："><a href="#5-3-1-条件概率：" class="headerlink" title="5.3.1 条件概率："></a>5.3.1 条件概率：</h3><script type="math/tex; mode=display">P(A|B)=\frac{P(A\cap B)}{P(B)};P(B|A)=\frac{P(A\cap B)}{P(A)}</script><p>$P(A|B): $ 事件A在事件B发生的条件下发生的概率。条件概率表示为$P(A|B)$</p><h3 id="5-3-2-贝叶斯定理："><a href="#5-3-2-贝叶斯定理：" class="headerlink" title="5.3.2 贝叶斯定理："></a>5.3.2 贝叶斯定理：</h3><p>对于给出的待分类项（即特征属性的集合），求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。</p><script type="math/tex; mode=display">P(A|B) \Longrightarrow P(B|A) = \frac{P(A|B)P(B)}{P(A)} \Longrightarrow P(B|A)</script><h3 id="5-3-3-朴素贝叶斯分类器："><a href="#5-3-3-朴素贝叶斯分类器：" class="headerlink" title="5.3.3 朴素贝叶斯分类器："></a>5.3.3 朴素贝叶斯分类器：</h3><ul><li><p>将每个属性和类标签视为随机变量</p></li><li><p>给出一个带有属性$（A_1, A_2, …, A_n）$的记录</p><ul><li><p>Goal: 预测类别$C$</p></li><li><p>寻找一个可以使得$P(C|A_1,A_2,…,A_n)$最大化的$C$值</p></li><li><script type="math/tex; mode=display">P(C|A_1,A_2,...,A_n) = \frac{P(A_1,A_2,...,A_n|C)P(C)}{P(A_1,A_2,...,A_n)}</script></li><li><p>设属性$A_i$之间相互独立，便有</p><script type="math/tex; mode=display">P(A_1, A_2, . . . , A_n|C_j) = \prod^{n}_iP(A_i|C_j)</script></li><li><p>而独立的$P(A_i|C_j)$是可估计的</p></li><li><p>则最大化$P(C|A_1,A_2,…,A_n)$等价于最大化$P(A_1,A_2,…,A_n|C)P(C)$，进一步等价于$P(C)\prod^{n}_iP(A_i|C)$</p><ul><li><p>若某个$i$所对应的条件概率为0，则根据上式整个表达式将为0</p></li><li><p>原始概率为</p><script type="math/tex; mode=display">P(A_i|C) = \frac{N_{iC}}{N_C}</script></li><li><p>应用Laplace Smoothing：</p><script type="math/tex; mode=display">P(A_i|C) = \frac{N_{iC}+a}{N_C+a\times(\#A_i)}</script><p>其中$(#A_i)$是$A_i$可能属于的总类数、$α&gt;0$是一个参数。为了简单起见，我们将始终设定$α=1$</p></li></ul></li></ul></li></ul><h2 id="5-4-Artificial-Neural-Networks-ANN’s"><a href="#5-4-Artificial-Neural-Networks-ANN’s" class="headerlink" title="5.4 Artificial Neural Networks (ANN’s)"></a>5.4 Artificial Neural Networks (ANN’s)</h2><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203115345413.png" alt="image-20221203115345413"></p><ul><li><p>$Z_i = σ(α_i + β_i^T X), i = 1,2,3$</p></li><li><p>$X = (X_1,X_2,X_3,X_4)^T$</p></li><li><p>$σ$：sigmoid函数，通常取$σ(x)=\frac{1}{1+e^{-x}}$</p></li><li><p>$σ$：激活函数（还有其他流行的激活函数）：</p><ul><li>如ReLU ： $σ(x) = max\{0, x\}$</li><li>$Y_i = σ(γ_i + δ_i^TZ), i = 1,2$.        $z=(z_1, z_2, z_3)$</li><li>本质上是一个优化问题。将$Y_i$’s适合于数据</li><li>但不是凸优化问题，因此并不能找到全局最优值</li><li>由于有多层隐藏层；因此容易造成过拟合</li></ul><h3 id="5-4-1-The-empirical-risk-function"><a href="#5-4-1-The-empirical-risk-function" class="headerlink" title="5.4.1 The empirical risk function"></a>5.4.1 The empirical risk function</h3><script type="math/tex; mode=display">l(W_1, b_1, W_2, b_2) = \frac{1}{m}\sum^{m}_{j=1}V(Y^j, Y(X^j; W_1, b_1, W_2, b_2))</script><p>其中，函数$V$是衡量$Y^j$和$Y(X^j;W_1,b_1,W_2,b_2)$之间 “距离 “的损失函数。</p><p>For example, if $\alpha = \begin{pmatrix} \alpha_1\\\alpha_2\end{pmatrix}\$ and $\beta = \begin{pmatrix} \beta_1\\\beta_2\end{pmatrix}\$ ，可以应用如下损失函数</p></li></ul><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203122616849.png" alt="image-20221203122616849"></p><ul><li>通过最小化$l(W_1^<em>,b_1^</em>,W_2^<em>,b_2^</em>)$的最优参数<script type="math/tex; mode=display">(W_1^*,b_1^*,W_2^*,b_2^*) = argmin l(W_1,b_1,W_2,b_2)</script></li></ul><p>通常情况下，最小化问题是通过梯度下降法解决的</p><h3 id="5-4-2-The-Backpropagation-Algorithm"><a href="#5-4-2-The-Backpropagation-Algorithm" class="headerlink" title="5.4.2 The Backpropagation Algorithm"></a>5.4.2 The Backpropagation Algorithm</h3><ul><li><p>现存问题：给定一个多元函数$f(x)\ with\ x\in\R^p $，求梯度$D_xf(x)$</p></li><li><p>Motivation：利用梯度下降法（gradient descent algorithm）最小化经验损失函数（empirical loss function），</p><script type="math/tex; mode=display">f(x)=l(W_1,b_1,W_2,b_2),\\x = (W_1^1,W_1^2,W_1^3,W_1^4,b_1,W_2^1,W_2^2,b_2^1,b_2^2) \in \R^9</script></li></ul><h2 id="5-5-Support-Vector-Machines"><a href="#5-5-Support-Vector-Machines" class="headerlink" title="5.5 Support Vector Machines"></a>5.5 Support Vector Machines</h2><p><a href="file:///C:/Users/NOID_/AppData/Local/Temp/MicrosoftEdgeDownloads/a8d49d85-2776-4a44-94c3-f942c2cb56e1/Intro2SVM.pdf">Intro2SVM.pdf</a></p><h3 id="5-5-1-Contour-lines-and-hyperplanes-of-linear-functions"><a href="#5-5-1-Contour-lines-and-hyperplanes-of-linear-functions" class="headerlink" title="5.5.1 Contour lines and hyperplanes of linear functions"></a>5.5.1 Contour lines and hyperplanes of linear functions</h3><p>通常将函数$f(x)=X^T\beta+\beta_0$称之为线性方程</p><p>For example：设$x = (x_1,x_2)^T$,$\beta = (\beta_1,\beta_2)^T$,则：</p><script type="math/tex; mode=display">f(x)=x^T+\beta_0=\beta_1x_1+\beta_2x_2+\beta_0</script><ul><li>超平面：<ul><li>每个超平面都可以表示为一个值为零的线性函数的轮廓</li></ul></li><li>margin：</li><li>support vector：</li></ul><h3 id="5-5-2-Support-Vector-Machine"><a href="#5-5-2-Support-Vector-Machine" class="headerlink" title="5.5.2 Support Vector Machine"></a>5.5.2 Support Vector Machine</h3><p>上述问题可以转化为：</p><script type="math/tex; mode=display">min\abs\beta\abs\,\\ subject\ to\ y_i(x_i^T\beta+\beta_0)\geqslant1,i=1,2,...,m</script><p>目标函数为凸函数，限制集也为凸函数，在大多数实际情况下，点集也不是线性可分离的。</p><p>则通常的SVM可定义为：</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203210908752.png" alt="image-20221203210908752"></p><p>$\phi_h(t)=max\{1-t,0\}$被叫做边缘损失函数（hinge loss function）</p><h3 id="5-5-3-Kernel-SVM"><a href="#5-5-3-Kernel-SVM" class="headerlink" title="5.5.3 Kernel SVM"></a>5.5.3 Kernel SVM</h3><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203211159157.png" alt="image-20221203211159157"></p><h2 id="5-6-Logistic-Regressin"><a href="#5-6-Logistic-Regressin" class="headerlink" title="5.6 Logistic Regressin"></a>5.6 Logistic Regressin</h2><p>可以用于<strong>多元</strong>和<strong>二元分类</strong>问题</p><ul><li><p>二元数据（Binary data）</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203212020766.png" alt="image-20221203212020766"></p></li></ul><p>$\phi(t) = \frac{e^t}{1+e^t}$   (Sigmoid function)</p><h3 id="5-6-1-Logistic-Regression"><a href="#5-6-1-Logistic-Regression" class="headerlink" title="5.6.1 Logistic Regression"></a>5.6.1 Logistic Regression</h3><ul><li><p>对数据进行二分类：$\{(x_i,y_i)\}^m_{i=1}$</p></li><li><p>$x_i\in\R^p$</p></li><li><p>$y_i\in\{-1,1\}$</p></li><li><p>逻辑回归算法+正则化参数定义如下：</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203223605266.png" alt="image-20221203223605266" style="zoom:50%;" /></p></li><li><p>预测函数$f<em>(x)$为$x^T\beta^</em>$内积的指示函数</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203223737416.png" alt="image-20221203223737416" style="zoom:50%;" /></p></li><li><p>设：</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203223837335.png" alt="image-20221203223837335" style="zoom:50%;" /></p><p>则逻辑回归问题转化为最小化$\xi(\beta)+\lambda||\beta||^2$,该问题为凸优化</p></li><li><p>利用Sigmoid函数$\phi(t)$ rewrite $\xi(\beta)$</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203225051631.png" alt="image-20221203225051631" style="zoom:50%;" /></p></li><li><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203225111101.png" alt="image-20221203225111101" style="zoom:50%;" /></p></li><li><p>若设$\lambda=0$，则逻辑回归整体会过拟合</p></li><li><p>若数据为Separable，则$\xi(\beta)$ 可能难以取得最小值</p><ul><li>若$y_ix_i^T\beta&gt;0$，则 $\xi(\beta)$没有最小值</li><li><strong>Proof:</strong> 若$\beta^<em>$为最小值，则 $\xi(2\beta^</em>)&lt;\xi(\beta^*)$</li></ul></li><li><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203232434814.png" alt="image-20221203232434814" style="zoom:50%;" /></p></li><li><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203232457480.png" alt="image-20221203232457480" style="zoom:50%;" /></p></li></ul><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203232705459.png" alt="image-20221203232705459" style="zoom:50%;" /></p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203232719841.png" alt="image-20221203232719841" style="zoom:50%;" /></p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203232742134.png" alt="image-20221203232742134" style="zoom:50%;" /></p><h2 id="5-7-Evaluating-the-Performance-of-Classifiers"><a href="#5-7-Evaluating-the-Performance-of-Classifiers" class="headerlink" title="5.7 Evaluating the Performance of Classifiers"></a>5.7 Evaluating the Performance of Classifiers</h2><h3 id="5-7-1-Score-based-classifier"><a href="#5-7-1-Score-based-classifier" class="headerlink" title="5.7.1 Score-based classifier"></a>5.7.1 Score-based classifier</h3><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221205094941810.png" alt="image-20221205094941810" style="zoom:50%;" /></p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221203233510289.png" alt="image-20221203233510289" style="zoom:50%;" /></p><h3 id="5-7-2-ROC-Curve"><a href="#5-7-2-ROC-Curve" class="headerlink" title="5.7.2 ROC Curve"></a>5.7.2 ROC Curve</h3><ul><li><p>对于$λ=-∞$，分类器预测一切为正</p><ul><li>根据前面的定义，$TPR = TPR(λ) = 1，而FPR = FPR(λ) = 1$（无用的预测器）</li></ul></li><li><p>对于$λ = ∞$，分类器预测一切为负</p></li><li>那么$T P R = T P R(λ) = 0，F P R = F P R(λ) = 0$（无用的预测器）</li><li>我们看到，当$λ从∞变为-∞时，F P R(λ)和T P R(λ)$都会增加</li><li>把假阳率$（FPR）当x轴，真阳率（TPR）当y轴$画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。<br>需要注意的是，ROC曲线必定起于$（0，0），止于（1，1）$。</li><li>因为，当全都判断为阴性(-)时，就是$（0，0）$；全部判断为阳性(+)时就是$（1，1）$。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方。</li></ul><h4 id="5-7-2-1-AUC"><a href="#5-7-2-1-AUC" class="headerlink" title="5.7.2.1 AUC"></a>5.7.2.1 AUC</h4><ul><li>AUC被定义为ROC曲线下的面积。AUC的得分范围为[0，1]</li><li>AUC可以用来评估一个分类器的性能</li><li>AUC socre接近1表示分类效果较好</li><li>AUC接近0表示分类效果较好，但分类方法应当相反</li><li>AUC接近0.5表示分类效果较差。特别当$AUC≈0.5$意味着该分类器的性能几乎与随机猜测相同<ul><li>(Random Guess 也是一个分类器，但是是最差的分类器)</li></ul></li></ul><h1 id="Chapter-6"><a href="#Chapter-6" class="headerlink" title="Chapter 6"></a>Chapter 6</h1><h1 id="Chapter-7"><a href="#Chapter-7" class="headerlink" title="Chapter 7"></a>Chapter 7</h1><h2 id="7-1-Dimension-Reduction"><a href="#7-1-Dimension-Reduction" class="headerlink" title="7.1 Dimension Reduction"></a>7.1 Dimension Reduction</h2><h3 id="7-1-1-Principal-Component-Analysis-PCA"><a href="#7-1-1-Principal-Component-Analysis-PCA" class="headerlink" title="7.1.1 Principal Component Analysis (PCA)"></a>7.1.1 Principal Component Analysis (PCA)</h3><p><strong>Step 1:</strong> data normalization</p><p><strong>Step 2:</strong> singular value decomposition (SVD)</p><p><strong>Step 3:</strong> linear transform to achieve dimension reduction</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221206100035668.png" alt="image-20221206100035668"></p><h2 id="7-2-Cluster-Analysis"><a href="#7-2-Cluster-Analysis" class="headerlink" title="7.2 Cluster Analysis"></a>7.2 Cluster Analysis</h2><h3 id="7-2-1-K-means-Clustering"><a href="#7-2-1-K-means-Clustering" class="headerlink" title="7.2.1 K-means Clustering"></a>7.2.1 K-means Clustering</h3><p>Step1：选择K个点作为初始中心点<br>Step2：通过将所有的点分配给最接近的中心点形成K个集群。<br>Step3：重新计算每个簇的中心点。如果所有计算出来的中心点与前面Step2的结果相同，则停止，否则重新执行Step2。</p><h4 id="7-2-1-1-Choosing-the-right-K"><a href="#7-2-1-1-Choosing-the-right-K" class="headerlink" title="7.2.1.1 Choosing the right K"></a>7.2.1.1 Choosing the right K</h4><ul><li>WCSS (<strong>Within Cluster Sum of Squares</strong>)： 评估中心点K的方式</li></ul><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221206103838184.png" alt="image-20221206103838184" style="zoom:33%;" /></p><ul><li><p>The Elbow Method：利用The Elbow Method根据WCSS选择最佳K值</p><p>结果显示，在3次之后，WCSS没有明显下降，所以3次是最好的。</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221206110430866.png" alt="image-20221206110430866" style="zoom:50%;" /></p></li><li><p>Shortcomings</p><ul><li><p>多次迭代：有帮助，但概率不在你这边；</p></li><li><p>可能产生空聚类。例如，我们可以重新放置空簇的中心点，以便使下面的SSE最小化；</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221206112123894.png" alt="image-20221206112123894" style="zoom:50%;" /></p></li><li><p>当点分布在流形上但使用环境距离时，可能会得到不好的结果；</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221206112107419.png" alt="image-20221206112107419" style="zoom:50%;" /></p></li></ul></li></ul><h3 id="7-2-2-Hierarchical-Clustering"><a href="#7-2-2-Hierarchical-Clustering" class="headerlink" title="7.2.2 Hierarchical Clustering"></a>7.2.2 Hierarchical Clustering</h3><h4 id="7-2-2-1-Strength-of-Hierarchical-Clustering"><a href="#7-2-2-1-Strength-of-Hierarchical-Clustering" class="headerlink" title="7.2.2.1 Strength of Hierarchical Clustering"></a>7.2.2.1 Strength of Hierarchical Clustering</h4><ul><li><p>不需要假设聚类数量，但需要在特定的层次上切割以形成聚类。</p></li><li><p>树状图很可能对应于有意义的分类法</p></li><li>Hierarchical Clustering是一种多层次的聚类算法：<ul><li>计算接近度矩阵</li><li>让每个数据点单独成为一个集群</li><li><strong>Repeat：</strong><ul><li>合并最接近的两个集群</li><li>更新接近度矩阵（proximity matrix）</li></ul></li><li><strong>Until</strong>：只剩下一个单一的集群</li></ul></li></ul><h2 id="7-3-Gaussian-Mixture-Model-Clustering-Using-EM-Algorithm"><a href="#7-3-Gaussian-Mixture-Model-Clustering-Using-EM-Algorithm" class="headerlink" title="7.3 Gaussian Mixture Model Clustering Using EM Algorithm"></a>7.3 Gaussian Mixture Model Clustering Using EM Algorithm</h2><h3 id="7-3-1-Maximum-likelihood-estimation-MLE"><a href="#7-3-1-Maximum-likelihood-estimation-MLE" class="headerlink" title="7.3.1 Maximum likelihood estimation (MLE)"></a>7.3.1 Maximum likelihood estimation (MLE)</h3><h4 id="7-3-1-1-Likelihood-Function"><a href="#7-3-1-1-Likelihood-Function" class="headerlink" title="7.3.1.1 Likelihood Function"></a>7.3.1.1 Likelihood Function</h4><p>​    令$X_1，X_2，…，X_n$是随机分布的样本，其特征为 $p(x|θ)$</p><ul><li><p>其中$θ$是一个未知参数;</p></li><li><p>$x_1, … … , x_n$是观测值</p></li><li><p>则似然函数的定义为：</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221206162104316.png" alt="image-20221206162104316" style="zoom:50%;" /></p></li></ul><h4 id="7-3-1-2-Log-likelihood-Function"><a href="#7-3-1-2-Log-likelihood-Function" class="headerlink" title="7.3.1.2 Log-likelihood Function"></a>7.3.1.2 Log-likelihood Function</h4><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221206162151305.png" alt="image-20221206162151305" style="zoom:50%;" /></p><ul><li><p>如果参数$θ使L(θ)$最大化，它也会使$l(θ)$最大化；</p></li><li><p>密度函数$p(x|θ)$是由参数$θ$决定的；在满足条件$θ$的情况下，不应该被当作条件密度函数(conditional density function) ；</p></li><li><p>参数$θ$不是随机的；</p></li></ul><h4 id="7-3-1-3-maximum-likelihood-estimator-MLE"><a href="#7-3-1-3-maximum-likelihood-estimator-MLE" class="headerlink" title="7.3.1.3 maximum likelihood estimator, MLE"></a>7.3.1.3 maximum likelihood estimator, <strong>MLE</strong></h4><p>最大似然估计器（MLE）$\hatθ_{mle} = \hatθ_{mle}(X_1, … , X_n)$被定义为是使似然函数 $L(θ)$最大化的参数（或者等同于使对数似然函数 $l(θ)$最大化）</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221206170004672.png" alt="image-20221206170004672" style="zoom:50%;" /></p><p><strong>Remark</strong></p><p>$\hatθ$是样本 $\hatθ(X_1，…，X_n)$的一个函数，因此它是一个随机变量</p><p>根据我们之前的定义，MLE $\hatθ$是一个统计量，也被称为点估计器。</p><p><strong>Remark</strong></p><p>如果未知分布$p$是由超过一个的参数指定的。$ p = p(x|θ_1, . . , θ_k)$，那么似然函数定义为</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221206172155537.png" alt="image-20221206172155537" style="zoom:50%;" /></p><p>而对数似然函数被定义为$l(θ_1, . . , θ_k) = logL(θ_1, . . , θ_k)$。MLE是使$L或l$最大化的向量$（\hatθ_1, …, \hatθ_k）$。</p><h5 id="How-to-find-the-MLE"><a href="#How-to-find-the-MLE" class="headerlink" title="How to find the MLE"></a>How to find the MLE</h5><p>求函数$L(θ)$的最大值的通常需要以下两个步骤：</p><p>Step1：通过下式求$θ$</p><script type="math/tex; mode=display">\frac{d}{dθ}L(θ) = 0</script><p>Step2：验证这个$θ$值是最大值的，方法是检查对这个θ值符合：</p><script type="math/tex; mode=display">\frac{d^2}{dθ^2}L(θ) \leq 0</script><p><strong>Remark</strong></p><ul><li>方程（1）的解不一定是唯一的</li><li>严格地说，上述两个步骤只能找到局部最大值，而MLE应当是全局最大值</li><li>在方程（1）和（2）中，函数$L(θ)$可以改为$l(θ)=logL(θ)$，因为对数函数是严格单调递增</li></ul><h3 id="7-3-2-Numerical-Calculation"><a href="#7-3-2-Numerical-Calculation" class="headerlink" title="7.3.2 Numerical Calculation"></a>7.3.2 Numerical Calculation</h3><h3 id="7-3-3-Conditional-Probability"><a href="#7-3-3-Conditional-Probability" class="headerlink" title="7.3.3 Conditional Probability"></a>7.3.3 Conditional Probability</h3><h3 id="7-3-4-The-EM-Algorithm"><a href="#7-3-4-The-EM-Algorithm" class="headerlink" title="7.3.4 The EM Algorithm"></a>7.3.4 The EM Algorithm</h3><h2 id="7-4-Other-clustering-methods"><a href="#7-4-Other-clustering-methods" class="headerlink" title="7.4 Other clustering methods"></a>7.4 Other clustering methods</h2><h3 id="7-4-1-Prototype-based-Clustering"><a href="#7-4-1-Prototype-based-Clustering" class="headerlink" title="7.4.1 Prototype-based Clustering"></a>7.4.1 Prototype-based Clustering</h3><h1 id="Chapter-8"><a href="#Chapter-8" class="headerlink" title="Chapter 8"></a>Chapter 8</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Intro&quot;&gt;&lt;a href=&quot;#Intro&quot; class=&quot;headerlink&quot; title=&quot;Intro&quot;&gt;&lt;/a&gt;Intro&lt;/h1&gt;&lt;h2 id=&quot;0-1-Derivatives&quot;&gt;&lt;a href=&quot;#0-1-Derivatives&quot; class=&quot;he</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Machine Learning</title>
    <link href="http://example.com/2022/11/28/Machine-Learning/"/>
    <id>http://example.com/2022/11/28/Machine-Learning/</id>
    <published>2022-11-28T12:12:51.000Z</published>
    <updated>2022-11-28T12:12:51.096Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Leetcode刷题笔记</title>
    <link href="http://example.com/2022/11/11/Leetcode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"/>
    <id>http://example.com/2022/11/11/Leetcode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/</id>
    <published>2022-11-11T03:16:24.000Z</published>
    <updated>2022-11-24T16:09:56.256Z</updated>
    
    <content type="html"><![CDATA[<h1 id="剑指offer（第二版）"><a href="#剑指offer（第二版）" class="headerlink" title="剑指offer（第二版）"></a>剑指offer（第二版）</h1><h2 id="1-简单"><a href="#1-简单" class="headerlink" title="1. 简单"></a>1. 简单</h2><h3 id="1-1-数组"><a href="#1-1-数组" class="headerlink" title="1.1. 数组"></a>1.1. 数组</h3><h4 id="剑指-Offer-03-数组中重复的数字-力扣（LeetCode）"><a href="#剑指-Offer-03-数组中重复的数字-力扣（LeetCode）" class="headerlink" title="            剑指 Offer 03. 数组中重复的数字 - 力扣（LeetCode）"></a><a href="https://leetcode.cn/problems/shu-zu-zhong-zhong-fu-de-shu-zi-lcof/?favorite=xb9nqhhg">            剑指 Offer 03. 数组中重复的数字 - 力扣（LeetCode）</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findRepeatNumber</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dic = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> num <span class="keyword">in</span> dic:</span><br><span class="line">                <span class="keyword">return</span> num</span><br><span class="line">            dic.add(num)</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="1-2-链表"><a href="#1-2-链表" class="headerlink" title="1.2. 链表"></a>1.2. 链表</h3><h4 id="剑指-Offer-06-从尾到头打印链表"><a href="#剑指-Offer-06-从尾到头打印链表" class="headerlink" title="剑指 Offer 06. 从尾到头打印链表"></a><a href="https://leetcode.cn/problems/cong-wei-dao-tou-da-yin-lian-biao-lcof/">剑指 Offer 06. 从尾到头打印链表</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reversePrint</span>(<span class="params">self, head: ListNode</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        stack = []</span><br><span class="line">        <span class="keyword">while</span> head:</span><br><span class="line">            stack.append(head.val)</span><br><span class="line">            head = head.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> stack[::-<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h4 id="剑指-Offer-18-删除链表的节点-力扣（LeetCode）"><a href="#剑指-Offer-18-删除链表的节点-力扣（LeetCode）" class="headerlink" title="剑指 Offer 18. 删除链表的节点 - 力扣（LeetCode）"></a><a href="https://leetcode.cn/problems/shan-chu-lian-biao-de-jie-dian-lcof/?favorite=xb9nqhhg">剑指 Offer 18. 删除链表的节点 - 力扣（LeetCode）</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleteNode</span>(<span class="params">self, head: ListNode, val: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">        <span class="comment"># 若第一个节点就是要删除的节点，则返回第二个节点</span></span><br><span class="line">        <span class="keyword">if</span> head.val == val:</span><br><span class="line">            <span class="keyword">return</span> head.<span class="built_in">next</span></span><br><span class="line">        <span class="comment"># 定义双指针</span></span><br><span class="line">        pre = head</span><br><span class="line">        cur = head.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">while</span> cur <span class="keyword">and</span> cur.val != val:</span><br><span class="line">            pre, cur = cur, cur.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">if</span> cur.val == val: </span><br><span class="line">            pre.<span class="built_in">next</span> = cur.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> head</span><br></pre></td></tr></table></figure><h4 id="剑指-Offer-22-链表中倒数第k个节点"><a href="#剑指-Offer-22-链表中倒数第k个节点" class="headerlink" title="剑指 Offer 22. 链表中倒数第k个节点"></a><a href="https://leetcode.cn/problems/lian-biao-zhong-dao-shu-di-kge-jie-dian-lcof/">剑指 Offer 22. 链表中倒数第k个节点</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getKthFromEnd</span>(<span class="params">self, head: ListNode, k: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">        pre, cur = head, head</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            cur = cur.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            pre, cur = pre.<span class="built_in">next</span>, cur.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> pre</span><br></pre></td></tr></table></figure><ol><li>制造一个间隔K位的距离：尾结点向后移动K位</li><li>尾结点退至最后一个，则头节点为倒数K个</li></ol><h4 id="剑指-Offer-24-反转链表-力扣（LeetCode）"><a href="#剑指-Offer-24-反转链表-力扣（LeetCode）" class="headerlink" title="            剑指 Offer 24. 反转链表 - 力扣（LeetCode）"></a><a href="https://leetcode.cn/problems/fan-zhuan-lian-biao-lcof/?favorite=xb9nqhhg">            剑指 Offer 24. 反转链表 - 力扣（LeetCode）</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        cur, pre = head, <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            tmp = cur.<span class="built_in">next</span></span><br><span class="line">            cur.<span class="built_in">next</span> = pre</span><br><span class="line">            pre = cur</span><br><span class="line">            cur = tmp </span><br><span class="line">        <span class="keyword">return</span> pre</span><br></pre></td></tr></table></figure><h4 id="剑指-Offer-25-合并两个排序的链表-力扣（LeetCode）"><a href="#剑指-Offer-25-合并两个排序的链表-力扣（LeetCode）" class="headerlink" title="剑指 Offer 25. 合并两个排序的链表 - 力扣（LeetCode）"></a><a href="https://leetcode.cn/problems/he-bing-liang-ge-pai-xu-de-lian-biao-lcof/?favorite=xb9nqhhg">剑指 Offer 25. 合并两个排序的链表 - 力扣（LeetCode）</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mergeTwoLists</span>(<span class="params">self, l1: ListNode, l2: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        temp = pre = ListNode(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">and</span> l2:</span><br><span class="line">            <span class="keyword">if</span> l1.val &lt; l2.val:</span><br><span class="line">                pre.<span class="built_in">next</span> = l1</span><br><span class="line">                l1 = l1.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pre.<span class="built_in">next</span> = l2</span><br><span class="line">                l2 = l2.<span class="built_in">next</span></span><br><span class="line">            pre = pre.<span class="built_in">next</span></span><br><span class="line">        <span class="comment"># 最后至多剩一个l1或者l2</span></span><br><span class="line">        pre.<span class="built_in">next</span> = l1 <span class="keyword">if</span> l1 <span class="keyword">else</span> l2</span><br><span class="line">        <span class="keyword">return</span> temp.<span class="built_in">next</span></span><br></pre></td></tr></table></figure><h4 id="剑指-Offer-52-两个链表的第一个公共节点-力扣（LeetCode）"><a href="#剑指-Offer-52-两个链表的第一个公共节点-力扣（LeetCode）" class="headerlink" title="剑指 Offer 52. 两个链表的第一个公共节点 - 力扣（LeetCode）"></a><a href="https://leetcode.cn/problems/liang-ge-lian-biao-de-di-yi-ge-gong-gong-jie-dian-lcof/?favorite=xb9nqhhg">剑指 Offer 52. 两个链表的第一个公共节点 - 力扣（LeetCode）</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getIntersectionNode</span>(<span class="params">self, headA: ListNode, headB: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        node1, node2 = headA, headB</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> node1 != node2:</span><br><span class="line">            node1 = node1.<span class="built_in">next</span> <span class="keyword">if</span> node1 <span class="keyword">else</span> headB</span><br><span class="line">            node2 = node2.<span class="built_in">next</span> <span class="keyword">if</span> node2 <span class="keyword">else</span> headA</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> node1</span><br></pre></td></tr></table></figure><h2 id="3-困难"><a href="#3-困难" class="headerlink" title="3. 困难"></a>3. 困难</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;剑指offer（第二版）&quot;&gt;&lt;a href=&quot;#剑指offer（第二版）&quot; class=&quot;headerlink&quot; title=&quot;剑指offer（第二版）&quot;&gt;&lt;/a&gt;剑指offer（第二版）&lt;/h1&gt;&lt;h2 id=&quot;1-简单&quot;&gt;&lt;a href=&quot;#1-简单&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Python 基础语法</title>
    <link href="http://example.com/2022/10/25/Python-%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"/>
    <id>http://example.com/2022/10/25/Python-%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/</id>
    <published>2022-10-25T05:08:14.000Z</published>
    <updated>2022-10-30T05:53:27.897Z</updated>
    
    <content type="html"><![CDATA[<p>A-20-B-30-C{-30-E-10-F-50-G</p><p>​                    {-10-D</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;A-20-B-30-C{-30-E-10-F-50-G&lt;/p&gt;
&lt;p&gt;​                    {-10-D&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>特征分解</title>
    <link href="http://example.com/2022/10/24/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3/"/>
    <id>http://example.com/2022/10/24/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3/</id>
    <published>2022-10-24T03:34:17.000Z</published>
    <updated>2022-10-25T06:42:43.304Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特征分解"><a href="#特征分解" class="headerlink" title="特征分解"></a>特征分解</h2><h2 id="奇异值分解-Singular-value-decomposition"><a href="#奇异值分解-Singular-value-decomposition" class="headerlink" title="奇异值分解 Singular value decomposition"></a>奇异值分解 Singular value decomposition</h2><p>奇异值分解，相比于特征分解可以在矩阵不为方阵的条件下对矩阵进行分解。</p><p>假设矩阵A是一个m*n的矩阵，则定义矩阵A的SVD为：</p><script type="math/tex; mode=display">A = U\Sigma V^T= (orthogonal)(diagonal)(orthogonal)</script><p>$其中U是m×m的矩阵，\Sigma 是m×n的对角矩阵（主对角线上为奇异值，除主对角线外皆为0），V是n×n矩阵；\\且U、V均满足U^TU = I,V^TV = I$ </p><p><img src="https://pic4.zhimg.com/v2-5ee98f8f3426b845bc1c5038ecd29593_r.jpg" alt="img"></p><p>矩阵U：$(AA^T)u_i=\lambda_iu_i$，讲m个特征向量v张成一个$m×m$的矩阵U</p><p>矩阵V：$(A^TA)v_i=\lambda_iv_i$，讲n个特征向量v张成一个$n×n$的矩阵V</p><p>奇异值矩阵$\Sigma$：$A=U\Sigma V^T \Rightarrow AV=U\Sigma V^TV \Rightarrow AV=U\Sigma\\\Rightarrow Av_i=\sigma_iu_i\Rightarrow\sigma_i=Av_i/u_i$ </p><h3 id="Proof："><a href="#Proof：" class="headerlink" title="Proof："></a>Proof：</h3><script type="math/tex; mode=display">A=U\Sigma V^T \Rightarrow  A^T=V\Sigma U^T \Rightarrow A^TA = V\Sigma U^TU\Sigma V^T = V\Sigma^2V^T</script><p> 上式证明了$AA^T$的特征向量即为SVD中的U矩阵，V矩阵同理；</p><p>$U^TU=I,\Sigma^T=\Sigma$；</p><p>同时可证明特征值和奇异值满足如下关系：$\sigma_i= \sqrt{\lambda_i}$，即通过求出$A^TA$的特征值取平方的方式求得奇异值</p><h2 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h2><h2 id="LASSO"><a href="#LASSO" class="headerlink" title="LASSO"></a>LASSO</h2><h2 id="Kernel-Methods"><a href="#Kernel-Methods" class="headerlink" title="Kernel Methods"></a>Kernel Methods</h2><p>高维空间比低维空间更易线性可分</p><p><strong>核函数: </strong> $ K:X×X\rightarrow R,\forall X,Z\in X,则K(X,Z)为核函数$</p><p><strong>正定核函数：</strong>$K:X×X\rightarrow R,\forall X,Z\in X,则K(X,Z)为核函数，如果\exists{\phi}：X\rightarrow R, \in H（希尔伯特空间）$</p><p><strong>Hilbert Space:</strong>  完备的，可能是无限维的，被赋予内积运算的线性空间</p><ul><li><p>线性空间：向量空间（加法和数乘）</p></li><li><p>完备：对极限操作是封闭的（即：若$K_n\in H，且\lim_{n \to \infty}\in H，则H为封闭空间$）</p></li><li><p>内积：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}&对称性:\ <f,g>=<g,f>\\&正定性:\ <f,f>\ge 0，当=成立时\Leftrightarrow f=0\\&线性:\ <r_1f_1+r_2f_2,g>=r_1<f_1,g>+r_2<f_2,g>\\\end{split}\end{equation}</script></li></ul><p><strong>$证明：K(X,Z)=&lt;\phi (X),\phi (Z)&gt; \Leftrightarrow Gram\ matrix 为半正定$</strong></p><ul><li><p>必要性证明：</p><p>$\because K(X,Z)=&lt;\phi(X),\phi(Z)&gt;,K(Z,X)=&lt;\phi(Z),\phi(X)&gt;$</p></li></ul><p>​        $\because 内积具有对称性质，即&lt;\phi(X),\phi(Z)&gt;=&lt;\phi(Z),\phi(X)&gt;$</p><p>​        $\therefore K(X,Z) = K(Z,X)$</p><p>​        $\therefore K(X,Z)满足对称性$    </p><ul><li><p>充分性证明：</p><p>$欲证Gram\ Matrix 为半正定矩阵，即证：\forall \alpha\in R^N, \alpha^T K \alpha \ge 0$</p><p><img src="C:\Users\NOID_\AppData\Roaming\Typora\typora-user-images\image-20221021145655547.png" alt="image-20221021145655547"></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>$高斯核：K(x_i,x_j)=\exp(-\frac{||x_i-x_j||^2}{2\gamma^2}),\gamma&gt;0(\gamma 为高斯核的带宽（width）)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gaussian 高斯核函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernelFunction</span>(<span class="params">xRow, xCol, gamma</span>):</span><br><span class="line">    K = np.zeros((xRow.size, xCol.size))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(xRow.size):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(xCol.size):</span><br><span class="line">            K[i,j] = np.exp(-(xRow[i] - xCol[j]) ** <span class="number">2</span> / gamma)</span><br><span class="line">    <span class="keyword">return</span> K</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">genDataExample2</span>(<span class="params">size, sigma</span>):</span><br><span class="line">    X = np.random.uniform(low = <span class="number">0</span>, high = <span class="number">10</span>, size = size)</span><br><span class="line">    Y = np.sin(X) + np.random.normal(size = size, scale = sigma)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;X&#x27;</span>: X, <span class="string">&#x27;Y&#x27;</span>: Y&#125;</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>AMA546 chapter 4</p><p><a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/132275334">深入浅出说说ridge regression - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/51431045">回归分析|笔记整理（A）——岭回归，主成分回归（上） - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/67986077">[深度概念]·K-Fold 交叉验证 (Cross-Validation)的理解与应用 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/weixin_38393494/article/details/112648438?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-112648438-blog-80584430.pc_relevant_3mothn_strategy_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-112648438-blog-80584430.pc_relevant_3mothn_strategy_recovery&amp;utm_relevant_index=2">(28条消息) 【笔记】普通交叉验证 (CV) ，广义交叉验证(GCV)，图像恢复正则化参数选择_木木mum的博客-CSDN博客_广义交叉验证</a></p><p><a href="https://www.bilibili.com/video/av34731384/?p=1&amp;vd_source=20d1ec2b241b91d79fba2e1488e1c7bc">机器学习-核方法（1）-背景介绍_哔哩哔哩_bilibili</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;特征分解&quot;&gt;&lt;a href=&quot;#特征分解&quot; class=&quot;headerlink&quot; title=&quot;特征分解&quot;&gt;&lt;/a&gt;特征分解&lt;/h2&gt;&lt;h2 id=&quot;奇异值分解-Singular-value-decomposition&quot;&gt;&lt;a href=&quot;#奇异值分解-Singu</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>分类算法 Classification</title>
    <link href="http://example.com/2022/10/24/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-Classification/"/>
    <id>http://example.com/2022/10/24/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-Classification/</id>
    <published>2022-10-24T03:33:12.000Z</published>
    <updated>2022-10-24T03:33:52.114Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分类算法-Classification"><a href="#分类算法-Classification" class="headerlink" title="分类算法 Classification"></a>分类算法 Classification</h1><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><h3 id="决策树-Decision-Trees"><a href="#决策树-Decision-Trees" class="headerlink" title="决策树 Decision Trees"></a>决策树 Decision Trees</h3><p>决策树是一种监督学习（Supervised Learning）</p><p>监督学习：给出一组样本，每个样本都有一组属性和一个分类结果，即分类结果已知；通过学习这些样本得到一个决策树，这个决策树能够对新的数据给出正确的分类。</p><h4 id="Hunt-算法"><a href="#Hunt-算法" class="headerlink" title="Hunt 算法"></a>Hunt 算法</h4><p>通过将训练记录相继划分成较纯的子集，以递归方式建立决策树</p><ul><li><p>Hunt算法没有全局最优解</p></li><li><p>贪心策略可以用于支持</p></li><li><p>指定测试条件的方法：</p><ul><li>属性类型：Nominal, ordinal, continuous</li><li>分割方式：2-way split, Multi-way split</li></ul></li><li><p>判断是否是最优分割的方式：随着划分过程的不断进行，我们希望决策树的分支节点包含的样本尽可能属于同一类，即节点的纯度越来越高。纯度越高，类分布就越倾斜，划分结果越好。</p><ul><li><p>纯度 Impurity</p><p><img src="https://img-blog.csdnimg.cn/img_convert/ab540e9ca5ec29eb0d5025faf50883c4.png#pic_center" alt="img"></p><p>$当C_0类的数量占总数的90\%时该节点的纯度较大。 $</p><p>为了确定按某个属性划分的效果，我们需要比较划分前(父亲节点)和划分后(所有儿子节点)不纯度的降低程度，降低越多，划分的效果就越好。</p></li></ul></li></ul><h4 id="信息增益法-Information-gain-ID3"><a href="#信息增益法-Information-gain-ID3" class="headerlink" title="信息增益法 Information gain (ID3)"></a>信息增益法 Information gain (ID3)</h4><ul><li><strong>信息熵：</strong>一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件不确定的事就需要了解大量信息。<strong>熵(entropy)用于表示随机变量不确定性的度量</strong>，熵越大，表示不确定性越大。</li><li>$假设变量S有S_i(i=1,2,…,n)种情况，p_i表示第i种情况的概率，那么随机变量S的熵定义为：$</li></ul><script type="math/tex; mode=display">Entropy(S)=-\sum_{i=1}^{c} p_i\log_2p_i</script><p>Example：若14个值内部有9个“是”，5个“否”，则该数据“购买”的熵为</p><script type="math/tex; mode=display">Infor（D）=I（9,5）=-\frac{9}{14}\log_2{\frac{9}{14}}-\frac{5}{14}\log_2{\frac{5}{14}}=0.940</script><p>在随机变量S给定的条件下，随机变量A的条件熵Entropy(A|S)定义为：</p><script type="math/tex; mode=display">Entropy(A|S)=-\sum_{i=1}^{c} p_i·Entropy(A|S=s_i)</script><p><strong>信息增益</strong>：得知特征X的信息而使得分类Y的信息的不确定性减少的程度<em>（由于X的存在使得Y没那么不确定了）</em>。如果某个特征的信息增益比较大，就表示该特征对结果的影响较大，特征A对数据集S的信息增益表示为：</p><script type="math/tex; mode=display">Gain(S,A)=Entropy(S)-\sum_{v\in A}\frac{|S_v|}{|S|} Entropy(S_v)</script><div class="table-container"><table><thead><tr><th>青年人（5）</th><th>中年人（4）</th><th>老年人(5)</th></tr></thead><tbody><tr><td>购买（2）</td><td>购买（4）</td><td>购买（3）</td></tr><tr><td>不购买（3）</td><td>不购买（0）</td><td>不购买（2）</td></tr></tbody></table></div><p>$I(2,3)=$</p><p>$I(4,0)=$</p><p>$I(3,2)=$</p><p>$Infor_{age}(D)=\frac{5}{14}I(2,3)+\frac{4}{14}I(0,4)+\frac{5}{14}I(3,2)=0.694$</p><p>$Gain(age)=Info(D)-Info_age(D)=0.940-0694=0.246$</p><p>同理可得：</p><p>$Gain(age)=0.246$</p><p>$Gain(income)=0.029$</p><p>$Gain(fancy)=0.151$</p><p>$Gain(credit_rating)=0.048$</p><h3 id="朴素贝叶斯分类器-Naive-Bayes-Classifier"><a href="#朴素贝叶斯分类器-Naive-Bayes-Classifier" class="headerlink" title="朴素贝叶斯分类器 Naive Bayes Classifier"></a>朴素贝叶斯分类器 Naive Bayes Classifier</h3><ul><li><strong>贝叶斯派的核心思想：</strong>支持某项属性的事件发生得愈多，则该属性成立的可能性就愈大。</li></ul><p>​        贝叶斯方法建立在主观判断的基础上，你可以先估计一个值，然后根据客观事实不断修正。</p><ul><li><strong>贝叶斯公式：</strong>$P(A|B)=P(B|A)*P(A)/P(B)$<ul><li>$P(A|B):（后验概率）B事件发生的条件下A事件发生的条件概率$</li><li>$P(B|A):A事件发生的条件下B事件发生的条件概率$</li><li>$P(A):（先验概率）$</li></ul></li><li><p><strong>贝叶斯分类器：</strong></p></li><li><p><strong>朴素贝叶斯：</strong>假设特征之间相互独立</p></li></ul><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://zhuanlan.zhihu.com/p/30059442">决策树(Decision Tree)：通俗易懂之介绍 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/qq_43753724/article/details/125208042">(29条消息) 决策树(Hunt、ID3、C4.5、CART)_别团等shy哥发育的博客-CSDN博客_hunt决策树</a></p><p><a href="https://zhuanlan.zhihu.com/p/37575364">一文详解朴素贝叶斯(Naive Bayes)原理 - 知乎 (zhihu.com)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;分类算法-Classification&quot;&gt;&lt;a href=&quot;#分类算法-Classification&quot; class=&quot;headerlink&quot; title=&quot;分类算法 Classification&quot;&gt;&lt;/a&gt;分类算法 Classification&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Neutral Network</title>
    <link href="http://example.com/2022/03/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C+%E5%85%A8%E8%BF%9E%E6%8E%A5+Tahn+Relu/"/>
    <id>http://example.com/2022/03/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C+%E5%85%A8%E8%BF%9E%E6%8E%A5+Tahn+Relu/</id>
    <published>2022-03-29T16:00:00.000Z</published>
    <updated>2022-03-30T04:33:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h1><h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><p>​    神经网络由大量的神经元相互连接而成。每个神经元接受线性组合的输入后，最开始只是简单的线性加权，后来给每个神经元加上了非线性的激活函数，从而进行非线性变换后输出。每两个神经元之间的连接代表加权值，称之为<strong>权重（weight）</strong>。不同的权重和激活函数，则会导致神经网络不同的输出。</p><p>​    举个手写识别的例子，给定一个未知数字，让神经网络识别是什么数字。此时的神经网络的输入由一组被输入图像的像素所激活的输入神经元所定义。在通过非线性激活函数进行非线性变换后，神经元被激活然后被传递到其他神经元。重复这一过程，直到最后一个输出神经元被激活。从而识别当前数字是什么字。<script type="math/tex">\omega*X+b</script></p><p><img src="https://img-blog.csdn.net/20160716131107406" alt="img"></p><p>基本 $wx + b$ 的形式，其中</p><ul><li>$x1,x2$  表示输入向量</li><li>$w1,w2$  为权重，几个输入则意味着有几个权重，即每个输入都被赋予一个权重</li><li><em>b</em>为偏置bias</li><li>$g(z)$ 为激活函数</li><li><em>a</em> 为输出</li></ul><p>​    一开始为了简单，人们把激活函数定义成一个线性函数，即对于结果做一个线性变化，比如一个简单的线性激活函数是g(z) = z，输出都是输入的线性变换。后来实际应用中发现，线性激活函数太过局限，于是人们引入了非线性激活函数。</p><h2 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h2><p>​    将上文的神经元结合在一起便组成了神经网络，下图是一个三层神经网络。</p><p>​    输入层(样本数据)<br>​    隐藏层(隐藏层的层数和每层的神经元数目需要自己给定)， 如果有多个隐藏层，则意味着多个激活函数。<br>​    输出层(预测目标)</p><p><img src="https://img-blog.csdnimg.cn/20190101123204625.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTcyMzQ2NA==,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" /></p><p>​    同时，每一层都可能由单个或多个神经元组成，每一层的输出将会作为下一层的输入数据。比如下图中间隐藏层来说，隐藏层的3个神经元a1、a2、a3皆各自接受来自多个不同权重的输入（因为有x1、x2、x3这三个输入，所以a1 a2 a3都会接受x1 x2 x3各自分别赋予的权重，即几个输入则几个权重），接着，a1、a2、a3又在自身各自不同权重的影响下 成为的输出层的输入，最终由输出层输出最终结果。<img src="https://img-blog.csdn.net/20160703110336151" alt="img"></p><p>​    $a_i^j$ 表示第j层第i个单元的激活函数/神经元</p><p>​    $\Theta^{(j)}$ 表示从第j层映射到第j+1层的控制函数的权重矩阵 </p><p>​    此外，输入层和隐藏层都存在一个偏置（bias unit)，所以上图中也增加了偏置项：x0、a0。针对上图，有如下公式</p><script type="math/tex; mode=display">a_1^{(2)} = g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3)\\a_2^{(2)} = g(\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3)\\a_3^{(2)} = g(\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3)\\h_{\Theta}(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)}+\Theta_{11}^{(2)}a_1^{(2)}+\Theta_{12}^{(2)}a_2^{(2)}+\Theta_{13}^{(2)}a_3^{(2)})</script><h3 id="卷积神经网络的层级结构"><a href="#卷积神经网络的层级结构" class="headerlink" title="卷积神经网络的层级结构"></a>卷积神经网络的层级结构</h3><p><img src="https://img-blog.csdn.net/20160702205047459" alt="img"></p><p>最左边是：</p><p>​    数据输入层，在该层对数据做一些处理，比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）、归一化（把所有的数据都归一到同样的范围）、PCA/白化等等。CNN只对训练集做“去均值”这一步。</p><p>中间是：</p><p>​    CONV：卷积计算层，线性乘积 求和。<br>​    RELU：激励层<br>​    POOL：池化层，简言之，即取区域平均或最大。</p><p>最右边是：</p><p>​    FC：全连接层</p><h3 id="CNN的卷积计算层"><a href="#CNN的卷积计算层" class="headerlink" title="CNN的卷积计算层"></a>CNN的卷积计算层</h3><h4 id="卷积："><a href="#卷积：" class="headerlink" title="卷积："></a>卷积：</h4><p>​    对不同的数据窗口数据和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做<strong>内积</strong>（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。</p><p>​    如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据</p><p><img src="https://img-blog.csdn.net/20160702215705128" alt="img"></p><p>$4<em>0 + 0</em>0 + 0<em>0 + 0</em>0 + 0<em>1 + 0</em>1 + 0<em>0 + 0</em>1 + -4*2 = -8$</p><p>​    在CNN中，滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数：<br>　　a. 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。</p><p>​        b. 步长stride：决定滑动多少步可以到边缘。</p><p>​    　c. 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。 </p><p><img src="https://img-blog.csdn.net/20160705162205761" alt="img"></p><p><img src="https://img-blog.csdn.net/20160707204048899" alt="img"></p><h4 id="激励层与池化层"><a href="#激励层与池化层" class="headerlink" title="激励层与池化层"></a>激励层与池化层</h4><h5 id="激励："><a href="#激励：" class="headerlink" title="激励："></a>激励：</h5><p>实际梯度下降中，sigmoid容易饱和、造成终止梯度传递，且没有0中心化。因此可以尝试另外一个激活函数：ReLU 优点是收敛快，求梯度简单。</p><h5 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h5><p>​    下图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。<img src="https://img-blog.csdn.net/20160703121026432" alt="img"></p><h2 id="非线性函数-激活函数"><a href="#非线性函数-激活函数" class="headerlink" title="非线性函数(激活函数)"></a>非线性函数(激活函数)</h2><p>​    如果把神经元的非线性函数去掉的话，那么这个神经元可以写成：<strong>输出=输入*权值+偏置</strong>。这个就是<em>线性回归方程</em>，所以如果把神经网络的非线性函数去掉，那么<em>整个网络就是由多个线性回归</em>组成的。线性回归是线性方程，只能解决线性可分的问题。多个线性回归组成的结果无论经过多少的层的神经网络，其本质上也是线性方程。但现实中的很多需要解决的问题都是线性不可分的(异或问题)。<em>*所以为了解决这些线性不可分的问题，我们引入了非线性方程 g</em>，那么输出就变为了 <script type="math/tex">g(\omega*X+b)</script>。</p><h3 id="目前常用的非线性函数"><a href="#目前常用的非线性函数" class="headerlink" title="目前常用的非线性函数"></a>目前常用的非线性函数</h3><h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数:"></a>sigmoid函数:</h4><script type="math/tex; mode=display">S(x) = \frac{1}{1+e^{-x}}</script><p><img src="https://img-blog.csdnimg.cn/20190101125917773.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zOTcyMzQ2NA==,size_16,color_FFFFFF,t_70" alt="img"></p><p>​    sigmoid函数把求和的结果都变为(0,1)之间的值，压缩至0到1有何用处呢？用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。</p><h4 id="tanh函数："><a href="#tanh函数：" class="headerlink" title="tanh函数："></a>tanh函数：</h4><script type="math/tex; mode=display">tanh(x) = \frac{sinh(x)}{cosh(x)}=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><p><img src="https://img-blog.csdnimg.cn/2019010113063249.png" alt="img"></p><p>​    在具体应用中，tanh函数相比于Sigmoid函数往往更具有优越性，这主要是因为<strong>Sigmoid函数在输入处于[-1,1]之间时，函数值变化敏感</strong>，一旦接近或者超出区间就失去敏感性，处于饱和状态，影响神经网络预测的精度值。主要体现在计算梯度时。</p><h4 id="relu函数："><a href="#relu函数：" class="headerlink" title="relu函数："></a>relu函数：</h4><script type="math/tex; mode=display">f(x) = max(0,x)</script><p><img src="https://img-blog.csdnimg.cn/20190101131458349.png" alt="img"></p><p>​    relu函数并未像tanh函数和sigmoid函数一样，将求和的值限制在(0,1)之间。其阈值是[0,+∞]。</p><p>​    上面3个激活函数主要是用在从输入层到隐藏层以及隐藏层到隐藏层之间。</p><p>​    对于最后一层隐藏层到输出层之间，一般选用softmax函数进行归一化。</p><h4 id="softmax函数："><a href="#softmax函数：" class="headerlink" title="softmax函数："></a>softmax函数：</h4><script type="math/tex; mode=display">\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}}\\\\for j = 1,...,K</script><p>​    由于在用神经网络解决的问题大部分都是多分类问题，softmax是将多分类转化为概率的一个函数。如果预测的目标是二分类，则使用sigmoid进行二分类的概率转化。</p><p>上面表达式中的 $z^j$ = 隐藏层的输入<em>权重+偏置，</em>K*为预测目标的分类数量。</p><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>​        全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。</p><p>​        全连接层每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。</p><p>​        <img src="https://img-blog.csdnimg.cn/20190325192219666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxOTk3OTIw,size_16,color_FFFFFF,t_70" alt="img"></p><p>​    从左至右，一张彩色图片输入到网络，白色框是卷积层，红色是池化，蓝色是全连接层，棕色框是预测层。预测层的作用是将全连接层输出的信息转化为相应的类别概率，而起到分类作用。    </p><p>​    如果说卷积层、池化层和激活函数层（预测层）等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示<strong>（7x7x512）</strong>”映射到样本标记空间<strong>(三个全连接层)</strong>的作用。在实际使用中，全连接层可由卷积操作实现：</p><p>​    对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为$h<em>w$的全局卷积，h和w分别为前层卷积结果的高和宽<em>*（7x7）</em></em>。</p><p>​    全连接的核心操作就是矩阵向量乘积 $ y = Wx$</p><p>​    本质就是由一个特征空间线性变换到另一个特征空间。目标空间的任一维 (也就是隐层的一个 cell)都认为会受到源空间的每一维的影响。不考虑严谨，可以说，<strong>目标向量是源向量的加权和</strong>。</p><p>​    在 CNN 中，全连接常出现在最后几层，用于对前面设计的特征做加权和。比如 mnist，前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。<u>在 RNN 中，全连接用来把 embedding 空间拉到隐层空间，把隐层空间转回 label 空间等</u>。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://blog.csdn.net/qq_41997920/article/details/88803736?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_default&amp;utm_relevant_index=5">https://blog.csdn.net/qq_41997920/article/details/88803736?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2.pc_relevant_default&amp;utm_relevant_index=5</a></p><p><a href="https://y1ran.blog.csdn.net/article/details/81385159?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.topblog&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.topblog&amp;utm_relevant_index=6">https://y1ran.blog.csdn.net/article/details/81385159?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.topblog&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3.topblog&amp;utm_relevant_index=6</a></p><p><a href="https://blog.csdn.net/v_JULY_v/article/details/51812459?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164860614516782089398149%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164860614516782089398149&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-51812459.142">https://blog.csdn.net/v_JULY_v/article/details/51812459?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164860614516782089398149%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=164860614516782089398149&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-51812459.142</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;人工神经网络&quot;&gt;&lt;a href=&quot;#人工神经网络&quot; class=&quot;headerlink&quot; title=&quot;人工神经网络&quot;&gt;&lt;/a&gt;人工神经网络&lt;/h1&gt;&lt;h2 id=&quot;神经元&quot;&gt;&lt;a href=&quot;#神经元&quot; class=&quot;headerlink&quot; title=&quot;神经元</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hexo博客部署</title>
    <link href="http://example.com/2022/03/29/Hexo%E5%8D%9A%E5%AE%A2%E9%83%A8%E7%BD%B2/"/>
    <id>http://example.com/2022/03/29/Hexo%E5%8D%9A%E5%AE%A2%E9%83%A8%E7%BD%B2/</id>
    <published>2022-03-29T15:10:55.000Z</published>
    <updated>2022-03-30T02:26:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用HEXO实现个人博客"><a href="#使用HEXO实现个人博客" class="headerlink" title="使用HEXO实现个人博客"></a>使用HEXO实现个人博客</h1><p>Node.js 安装<br>npm -v<br>npm install -g cnpm —registry=<a href="https://registry.npm.taobao.org">https://registry.npm.taobao.org</a><br>cnpm install -g hexo-cli<br>hexo -v<br>C:\Users\21059165g&gt;mkdir My_Blog<br>hexo init<br>hexo s<br><a href="http://localhost:4000/">http://localhost:4000/</a><br>hexo n “post_name”<br>也可以创建md文件再复制到文件夹下<br>cd source/_posts</p><h2 id="将个人博客推到github"><a href="#将个人博客推到github" class="headerlink" title="将个人博客推到github"></a>将个人博客推到github</h2><p>C:\Users\21059165g\My_Blog&gt;cnpm install —save hexo-deployer-git<br>打开 _config.yml<br>deploy:<br>  type: git<br>  repo: <a href="https://github.com/HANE-iwnl/HANE-iwnl/_posts.github.io.git">https://github.com/HANE-iwnl/HANE-iwnl/_posts.github.io.git</a><br>  branch :master<br>hexo d</p><p>git clone <a href="https://github.com/litten/hexo-theme-yilia.git">https://github.com/litten/hexo-theme-yilia.git</a> themes/yilia</p><p>Hexo clean<br>Hexo g<br>Hexo s</p><h2 id="使hexo支持数学公式"><a href="#使hexo支持数学公式" class="headerlink" title="使hexo支持数学公式"></a>使hexo支持数学公式</h2><p>npm install hexo-math —save</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plugins:</span><br><span class="line">    - markdown-it-footnote</span><br><span class="line">    - markdown-it-sup</span><br><span class="line">    - markdown-it-sub</span><br><span class="line">    - markdown-it-abbr</span><br><span class="line">    - markdown-it-emoji</span><br><span class="line">    - hexo-math</span><br></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br><span class="line">  cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML</span><br><span class="line"># Han Support docs: https://hanzi.pro/</span><br></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: A Title</span><br><span class="line">date: 2020-02-08 10:39:55</span><br><span class="line">tags:</span><br><span class="line">- tag1</span><br><span class="line">- tag2</span><br><span class="line">categories:</span><br><span class="line">- parent</span><br><span class="line">- child</span><br><span class="line"></span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用HEXO实现个人博客&quot;&gt;&lt;a href=&quot;#使用HEXO实现个人博客&quot; class=&quot;headerlink&quot; title=&quot;使用HEXO实现个人博客&quot;&gt;&lt;/a&gt;使用HEXO实现个人博客&lt;/h1&gt;&lt;p&gt;Node.js 安装&lt;br&gt;npm -v&lt;br&gt;npm in</summary>
      
    
    
    
    
  </entry>
  
</feed>
